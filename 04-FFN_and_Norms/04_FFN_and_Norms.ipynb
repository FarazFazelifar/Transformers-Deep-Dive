{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sample sentence: I invited my foolish friend Jay around for tennis because I thought he'd make me look good.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "torch.set_grad_enabled(False)  # No gradients for this didactic walkthrough\n",
    "\n",
    "# Load standard Persian-English data\n",
    "with open(\"../.data/en_fa_train.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# We will use a real sentence for our token embeddings\n",
    "sample_idx = 0\n",
    "text = data[sample_idx][\"input\"]\n",
    "print(f\"Using sample sentence: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffn-intro",
   "metadata": {},
   "source": [
    "## 1. The Feed-Forward Network\n",
    "\n",
    "The Feed-Forward Network (FFN) is a simple MLP applied independently to each position. Its main job is to expand the state space and allow for non-linear feature mixing.\n",
    "\n",
    "Math: $\\mathrm{FFN}(x) = \\sigma(xW_1 + b_1)W_2 + b_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffn-impl",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        # W1: Expansion layer (d_model -> d_ff)\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        # W2: Projection layer (d_ff -> d_model)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_len, d_model)\n",
    "        return self.w_2(self.gelu(self.w_1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ln-intro",
   "metadata": {},
   "source": [
    "## 2. Layer Normalization (LayerNorm)\n",
    "\n",
    "LayerNorm stabilizes training by normalizing the feature dimension for each token. Unlike Batchnorm, it works identically during training and inference and is well-suited for varying sequence lengths.\n",
    "\n",
    "Math: $\\mathrm{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\odot \\gamma + \\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ln-impl",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Normalize across the feature dimension (last dim)\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, unbiased=False, keepdim=True)\n",
    "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.gamma * x_hat + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-intro",
   "metadata": {},
   "source": [
    "## 3. Advanced: Modern Variants\n",
    "\n",
    "Modern architectures (Llama, Gemma, Mistral) often replace standard LayerNorm with RMSNorm for speed, and replace GeLU/ReLU with SwiGLU for better parameter efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rmsnorm-intro",
   "metadata": {},
   "source": [
    "### 3.1 RMSNorm\n",
    "\n",
    "RMSNorm simply scales the input by its root-mean-square. It removes the \"centering\" (mean subtraction) step of LayerNorm.\n",
    "\n",
    "Math: $\\mathrm{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_i x_i^2 + \\epsilon}} \\odot \\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rmsnorm-impl",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Use root mean square for normalization\n",
    "        rms = torch.sqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return (x / rms) * self.gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiglu-intro",
   "metadata": {},
   "source": [
    "### 3.2 SwiGLU\n",
    "\n",
    "SwiGLU is a gated activation function that uses the Swish activation function (SiLU in PyTorch).\n",
    "\n",
    "Math: $\\mathrm{SwiGLU}(x, W, V) = \\mathrm{SiLU}(xW) \\otimes xV$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "swiglu-impl",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        # In SwiGLU, we project into two parallel paths (W and V)\n",
    "        self.w_gate = nn.Linear(d_model, d_ff, bias=False)\n",
    "        self.w_up = nn.Linear(d_model, d_ff, bias=False)\n",
    "        self.w_down = nn.Linear(d_ff, d_model, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Gated Linear Unit logic\n",
    "        gate = F.silu(self.w_gate(x))\n",
    "        up = self.w_up(x)\n",
    "        return self.w_down(gate * up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residual-intro",
   "metadata": {},
   "source": [
    "## 4. Residual Connections\n",
    "\n",
    "Finally, we wrap everything in residual connections to ensure gradient health.\n",
    "\n",
    "Math:  = $x + \\mathrm{Sublayer}(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "residual-impl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 10, 512])\n",
      "Output shape: torch.Size([1, 10, 512])\n",
      "Successfully computed residual FFN block.\n"
     ]
    }
   ],
   "source": [
    "# Demo of a single Transformer sub-block (Pre-norm)\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "\n",
    "ffn_sublayer = PositionWiseFFN(d_model, d_ff)\n",
    "norm = LayerNorm(d_model)\n",
    "\n",
    "x = torch.randn(1, 10, d_model)  # (batch, seq, d_model)\n",
    "\n",
    "# Standard Pre-norm architecture:\n",
    "# x = x + FFN(Norm(x))\n",
    "output = x + ffn_sublayer(norm(x))\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"Successfully computed residual FFN block.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f80295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers-Deep-Dive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
