{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b17e42a0",
   "metadata": {},
   "source": [
    "# English → Farsi Translation with Transformers\n",
    "\n",
    "This course teaches you to build a transformer model **from scratch** and train it to translate English sentences into Farsi. You'll learn every component—tokenization, embeddings, attention, FFN layers, encoder/decoder blocks—by implementing them yourself before seeing how production libraries do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488d3c4c",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Before we begin, ensure you have the necessary packages installed. Run the cell below (it's safe to run even if packages are already installed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bad37b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.13/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8940fef9",
   "metadata": {},
   "source": [
    "## Download the Helsinki-Persian Dataset\n",
    "\n",
    "The Helsinki-Persian-Opus-100 dataset contains 2,047 English-Farsi parallel sentences. We'll download it and cache it locally in the `.data/` folder (which is in `.gitignore` to keep the repository size small).\n",
    "\n",
    "Run the cell below to download and cache the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a30fcc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Helsinki-Persian-Opus-100 dataset...\n",
      "✓ Downloaded 2047 translation pairs\n",
      "\n",
      "Dataset columns: ['instruction', 'input', 'output']\n",
      "Example structure:\n",
      "  instruction: Translate this sentence from English to Persian.\n",
      "  input: Pack your stuff.\n",
      "  output: بند و بساطتو جمع کن.\n",
      "\n",
      "Caching dataset locally to .data/en_fa_train.jsonl...\n",
      "✓ Dataset cached successfully!\n",
      "✓ Downloaded 2047 translation pairs\n",
      "\n",
      "Dataset columns: ['instruction', 'input', 'output']\n",
      "Example structure:\n",
      "  instruction: Translate this sentence from English to Persian.\n",
      "  input: Pack your stuff.\n",
      "  output: بند و بساطتو جمع کن.\n",
      "\n",
      "Caching dataset locally to .data/en_fa_train.jsonl...\n",
      "✓ Dataset cached successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Create .data folder if it doesn't exist\n",
    "os.makedirs('.data', exist_ok=True)\n",
    "\n",
    "print(\"Downloading Helsinki-Persian-Opus-100 dataset...\")\n",
    "dataset = load_dataset(\"Maani/Helsinki-Persian-Opus-100\")\n",
    "train_data = dataset['train']\n",
    "\n",
    "print(f\"✓ Downloaded {len(train_data)} translation pairs\")\n",
    "\n",
    "# Inspect the structure\n",
    "print(f\"\\nDataset columns: {train_data.column_names}\")\n",
    "print(f\"Example structure:\")\n",
    "sample = train_data[0]\n",
    "for key, value in sample.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Save locally to .data/en_fa_train.jsonl for faster loading in future cells\n",
    "print(\"\\nCaching dataset locally to .data/en_fa_train.jsonl...\")\n",
    "with open('.data/en_fa_train.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in train_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(\"✓ Dataset cached successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c2a84",
   "metadata": {},
   "source": [
    "## NumPy Basics\n",
    "\n",
    "NumPy is the foundational library for numerical computing in Python. It provides efficient arrays and mathematical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c9cf3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array 1: [1 2 3 4 5]\n",
      "Array 2 shape: (3, 4)\n",
      "Array 4: [0 2 4 6 8]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Creating arrays\n",
    "arr1 = np.array([1, 2, 3, 4, 5])\n",
    "arr2 = np.zeros((3, 4))  # 3x4 matrix of zeros\n",
    "arr3 = np.ones((2, 3))   # 2x3 matrix of ones\n",
    "arr4 = np.arange(0, 10, 2)  # [0, 2, 4, 6, 8]\n",
    "\n",
    "print(\"Array 1:\", arr1)\n",
    "print(\"Array 2 shape:\", arr2.shape)\n",
    "print(\"Array 4:\", arr4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2e5ecb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition: [5 7 9]\n",
      "Multiplication: [ 4 10 18]\n",
      "Dot product: 32\n",
      "Sum: 6\n",
      "Mean: 2.0\n"
     ]
    }
   ],
   "source": [
    "# Basic operations\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "print(\"Addition:\", a + b)\n",
    "print(\"Multiplication:\", a * b)\n",
    "print(\"Dot product:\", np.dot(a, b))\n",
    "print(\"Sum:\", np.sum(a))\n",
    "print(\"Mean:\", np.mean(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4afa516",
   "metadata": {},
   "source": [
    "## PyTorch Basics\n",
    "\n",
    "PyTorch is a deep learning framework that provides tensors (similar to NumPy arrays) with GPU acceleration and automatic differentiation for neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5774ec6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor 1: tensor([1., 2., 3., 4., 5.])\n",
      "Tensor 2 shape: torch.Size([3, 4])\n",
      "Device (CPU/GPU): cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Creating tensors\n",
    "tensor1 = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "tensor2 = torch.zeros(3, 4)  # 3x4 tensor of zeros\n",
    "tensor3 = torch.ones(2, 3)   # 2x3 tensor of ones\n",
    "tensor4 = torch.arange(0, 10, 2)  # [0, 2, 4, 6, 8]\n",
    "\n",
    "print(\"Tensor 1:\", tensor1)\n",
    "print(\"Tensor 2 shape:\", tensor2.shape)\n",
    "print(\"Device (CPU/GPU):\", tensor1.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7858c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition: tensor([5., 7., 9.])\n",
      "Multiplication: tensor([ 4., 10., 18.])\n",
      "Dot product: tensor(32.)\n",
      "Sum: tensor(6.)\n",
      "Mean: tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "# Basic tensor operations\n",
    "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "t2 = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "print(\"Addition:\", t1 + t2)\n",
    "print(\"Multiplication:\", t1 * t2)\n",
    "print(\"Dot product:\", torch.dot(t1, t2))\n",
    "print(\"Sum:\", torch.sum(t1))\n",
    "print(\"Mean:\", torch.mean(t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac5b0e",
   "metadata": {},
   "source": [
    "## NumPy to PyTorch Conversion\n",
    "\n",
    "PyTorch tensors can be easily converted to and from NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b4e8853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy array: [1 2 3 4 5]\n",
      "PyTorch tensor: tensor([1, 2, 3, 4, 5])\n",
      "Tensor dtype: torch.int64\n",
      "\n",
      "Converted back to NumPy: [1. 2. 3.]\n",
      "NumPy dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# NumPy to PyTorch\n",
    "np_array = np.array([1, 2, 3, 4, 5])\n",
    "torch_tensor = torch.from_numpy(np_array)\n",
    "print(\"NumPy array:\", np_array)\n",
    "print(\"PyTorch tensor:\", torch_tensor)\n",
    "print(\"Tensor dtype:\", torch_tensor.dtype)\n",
    "\n",
    "# PyTorch to NumPy\n",
    "torch_tensor_float = torch.tensor([1.0, 2.0, 3.0])\n",
    "np_array_from_torch = torch_tensor_float.numpy()\n",
    "print(\"\\nConverted back to NumPy:\", np_array_from_torch)\n",
    "print(\"NumPy dtype:\", np_array_from_torch.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5ba7cd",
   "metadata": {},
   "source": [
    "## Key Differences: NumPy vs PyTorch\n",
    "\n",
    "| Feature | NumPy | PyTorch |\n",
    "|---------|-------|----------|\n",
    "| GPU Support | No | Yes |\n",
    "| Automatic Differentiation | No | Yes (requires `.requires_grad=True`) |\n",
    "| Deep Learning Frameworks | Not designed for it | Built for neural networks |\n",
    "| Speed (CPU) | Very fast | Comparable |\n",
    "| Ecosystem | Scientific computing | Deep learning and AI |\n",
    "\n",
    "Both are essential: NumPy for data manipulation and preprocessing, PyTorch for building and training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47536784",
   "metadata": {},
   "source": [
    "## Exploring the Translation Data\n",
    "\n",
    "Let's look at some actual translation examples to understand what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d3b3d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2047\n",
      "\n",
      "First 5 English → Farsi translation pairs:\n",
      "================================================================================\n",
      "\n",
      "1. English: I invited my foolish friend Jay around for tennis because I thought he'd make me look good.\n",
      "   Farsi:    دوست ابله ام جِی رو مهمون کردم تنیس، چون پنداشتم انگیزه ای می‌شه که من بهتر به چشم بیام.\n",
      "\n",
      "2. English: Pack your stuff.\n",
      "   Farsi:    بند و بساطتو جمع کن.\n",
      "\n",
      "3. English: Aunt Silvy, stop yelling!\n",
      "   Farsi:    عمه سیلوی، داد نزن!\n",
      "\n",
      "4. English: I need to get out of here.\n",
      "   Farsi:    باید از اینجا بزنم بیرون.\n",
      "\n",
      "5. English: Which means the mommy of the smartest physicist at the university is not my mommy as I had thought.\n",
      "   Farsi:    که یعنی مامانِ باهوش‌ترین فیزیکدانِ دانشگاه، اون‌طور که فکر می‌کردم، مامان من نیست.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Note: The 'instruction' field tells the model what to do (e.g., 'Translate this sentence from English to Persian.').\n",
      "In our transformer, we'll use 'input' (English) as the encoder input and 'output' (Farsi) as the decoder target.\n"
     ]
    }
   ],
   "source": [
    "# Load the cached dataset\n",
    "import json\n",
    "\n",
    "with open('../.data/en_fa_train.jsonl', 'r', encoding='utf-8') as f:\n",
    "    all_samples = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Total samples: {len(all_samples)}\\n\")\n",
    "\n",
    "# Display the first 5 translation pairs\n",
    "print(\"First 5 English → Farsi translation pairs:\")\n",
    "print(\"=\" * 80)\n",
    "for i in range(5):\n",
    "    sample = all_samples[i]\n",
    "    english = sample['input']\n",
    "    farsi = sample['output']\n",
    "    print(f\"\\n{i+1}. English: {english}\")\n",
    "    print(f\"   Farsi:    {farsi}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nNote: The 'instruction' field tells the model what to do (e.g., 'Translate this sentence from English to Persian.').\")\n",
    "print(\"In our transformer, we'll use 'input' (English) as the encoder input and 'output' (Farsi) as the decoder target.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feba0239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers-Deep-Dive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
